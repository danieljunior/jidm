{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "mu, sigma = 0, 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# STS datasets generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## An√°lise dos dados de precedentes do STJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Levando em considera√ß√£o que cada jurisprud√™ncia do conjunto de dados ter√° no m√≠nimo 5 precedentes, utilizando a f√≥rmula de combina√ß√£o ùê∂(ùëõ,ùëü)=ùëõ!/(ùëü!(ùëõ‚àíùëü)!), onde n=10 e r=2, temos que para cada jurisprud√™ncia podemos gerar no m√≠nimo 10 pares de precedentes sem repeti√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stj = pd.read_csv('data/raw_text/jurisprudencias_stj_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de pares de precedentes da mesma Jurisprud√™ncia gerados:  18217\n"
     ]
    }
   ],
   "source": [
    "stj_sts_data = []\n",
    "for group_name, tema_group in stj.groupby('TEMA'):\n",
    "    pares = list(set(itertools.combinations(tema_group.EMENTA, 2)))\n",
    "    noise = np.random.normal(mu, sigma, len(pares))\n",
    "    scores = [4.5]*len(pares)+noise\n",
    "    for i, par in enumerate(pares):\n",
    "        stj_sts_data.append([par[0], par[1], scores[i], 4])\n",
    "\n",
    "precedentes_mesma_jurisprudencia = len(stj_sts_data)\n",
    "print('Total de pares de precedentes da mesma Jurisprud√™ncia gerados: ', precedentes_mesma_jurisprudencia)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Agora preciso gerar tamb√©m pares de precedentes que n√£o s√£o similares. Para isso posso usar a informa√ß√£o da Mat√©ria de cada precedente, e gerar pares entre Mat√©rias diferentes, o que garante que a similaridade entre os precedentes seja praticamente nula. Como os pares n√£o similares gerados por essa abordagem ser√° maior que a quantidade de pares da mesma jurisprud√™ncia gerados pelo passo anterior, uso a rela√ß√£o da quantidade de pares precedentes da mesma jurisprud√™ncia, e os pares de Mat√©rias diferentes para manter um conjunto de dados balanceado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de pares de diferentes Mat√©rias:  21\n",
      "Total de pares a serem usados a partir de cada par de Mat√©ria:  867\n",
      "Total de pares at√© o momento:  36424\n"
     ]
    }
   ],
   "source": [
    "#gero pares de mat√©rias diferentes\n",
    "pares_materias = list(set(itertools.combinations(stj.MATERIA.unique().tolist(), 2)))\n",
    "print('Total de pares de diferentes Mat√©rias: ' , len(pares_materias))\n",
    "total_por_par_materias = int(precedentes_mesma_jurisprudencia/len(pares_materias))\n",
    "print('Total de pares a serem usados a partir de cada par de Mat√©ria: ', total_por_par_materias)\n",
    "for materias_par in list(set(itertools.combinations(stj.MATERIA.unique().tolist(), 2))):\n",
    "    #recupero as ementas das duas diferentes mat√©rias\n",
    "    precedentes_materias = [stj[stj.MATERIA == materia].EMENTA.tolist() for materia in materias_par]\n",
    "    # gero um produto cartesiano entre as ementas das diferentes mat√©rias\n",
    "    pares = list(itertools.product(*precedentes_materias))[:total_por_par_materias]\n",
    "    noise = np.random.normal(mu, sigma, len(pares))\n",
    "    scores = [0.5]*len(pares)+noise\n",
    "    for i, par in enumerate(pares):\n",
    "        stj_sts_data.append([par[0], par[1], scores[i], 0])\n",
    "print('Total de pares at√© o momento: ', len(stj_sts_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pares_natureza = []\n",
    "for group_name, natureza_group in stj.groupby('NATUREZA'):\n",
    "    for temas_pares in list(set(itertools.combinations(natureza_group.TEMA.unique(), 2))):\n",
    "        precedentes_temas = [natureza_group[natureza_group.TEMA == tema].EMENTA.tolist() \n",
    "                                for tema in temas_pares]\n",
    "        pares = list(itertools.product(*precedentes_temas))\n",
    "        noise = np.random.normal(mu, sigma, len(pares))\n",
    "        scores = [3]*len(pares)+noise\n",
    "        for i, par in enumerate(pares):\n",
    "            pares_natureza.append([par[0], par[1], scores[i], 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de pares at√© o momento 54641\n"
     ]
    }
   ],
   "source": [
    "samples_pares_natureza = random.sample(pares_natureza, min(precedentes_mesma_jurisprudencia, len(pares_natureza)))\n",
    "stj_sts_data = stj_sts_data + samples_pares_natureza\n",
    "print('Total de pares at√© o momento', len(stj_sts_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stj_sts_df = pd.DataFrame(stj_sts_data, columns=['sentence_A','sentence_B','score','range'])\n",
    "stj_sts_df = stj_sts_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(stj_sts_df, \n",
    "                            test_size=0.3, \n",
    "                            stratify=stj_sts_df.range,\n",
    "                            shuffle=True,\n",
    "                            random_state=42)\n",
    "test, valid = train_test_split(test, \n",
    "                            test_size=0.3, \n",
    "                            stratify=test.range,\n",
    "                            shuffle=True,\n",
    "                            random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stj_sts_df.loc[train.index.values,'SPLIT'] = 'TRAIN'\n",
    "stj_sts_df.loc[test.index.values,'SPLIT'] = 'TEST'\n",
    "stj_sts_df.loc[valid.index.values,'SPLIT'] = 'VALID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              sentence_A  \\\n",
      "0      ADMINISTRATIVO. AGRAVO REGIMENTAL. RECURSO ORD...   \n",
      "1      ADMINISTRATIVO E PROCESSUAL CIVIL. SEGUNDOS EM...   \n",
      "2      ADMINISTRATIVO. RECURSO ORDIN√ÅRIO EM MANDADO D...   \n",
      "3      ADMINISTRATIVO. RECURSO ORDIN√ÅRIO EM MANDADO D...   \n",
      "4      ADMINISTRATIVO. AGRAVO REGIMENTAL. RECURSO ORD...   \n",
      "...                                                  ...   \n",
      "54636  CIVIL E CONSUMIDOR. RECURSO ESPECIAL. CONTRATO...   \n",
      "54637  AGRAVO INTERNO NO RECURSO ESPECIAL. DESCONTO I...   \n",
      "54638  ADMINISTRATIVO E PROCESSUAL CIVIL. RESPONSABIL...   \n",
      "54639  AGRAVO  INTERNO  NO  AGRAVO  EM RECURSO ESPECI...   \n",
      "54640  Direito civil. Previd√™ncia privada. Benef√≠cios...   \n",
      "\n",
      "                                              sentence_B     score  range  \\\n",
      "0      ADMINISTRATIVO. RECURSO ORDIN√ÅRIO EM MANDADO D...  4.676405      4   \n",
      "1      ADMINISTRATIVO. RECURSO ORDIN√ÅRIO EM MANDADO D...  4.540016      4   \n",
      "2      ADMINISTRATIVO. RECURSO ORDIN√ÅRIO EM MANDADO D...  4.597874      4   \n",
      "3      ADMINISTRATIVO E PROCESSUAL CIVIL. SEGUNDOS EM...  4.724089      4   \n",
      "4      ADMINISTRATIVO E PROCESSUAL CIVIL. SEGUNDOS EM...  4.686756      4   \n",
      "...                                                  ...       ...    ...   \n",
      "54636  PROCESSO  CIVIL.  AGRAVO REGIMENTAL. AGRAVO EM...  3.138156      3   \n",
      "54637  AGRAVO REGIMENTAL NO RECURSO ESPECIAL. TRANSPO...  3.049909      3   \n",
      "54638  AGRAVO REGIMENTAL NO RECURSO ESPECIAL. MILITAR...  3.099351      3   \n",
      "54639  AGRAVO INTERNO NO RECURSO ESPECIAL - A√á√ÉO DE O...  2.934198      3   \n",
      "54640  RECURSO ESPECIAL. PROCESSUAL PENAL. CRIMES DE ...  2.851012      3   \n",
      "\n",
      "       SPLIT  \n",
      "0      TRAIN  \n",
      "1       TEST  \n",
      "2      TRAIN  \n",
      "3       TEST  \n",
      "4      TRAIN  \n",
      "...      ...  \n",
      "54636  TRAIN  \n",
      "54637  TRAIN  \n",
      "54638  TRAIN  \n",
      "54639  TRAIN  \n",
      "54640  VALID  \n",
      "\n",
      "[54641 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(stj_sts_df)\n",
    "stj_sts_df.to_csv('data/sts/stj_sts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/sts/stj_sts.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_70/2068067650.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m stj_sts = pd.read_csv(\"../data/sts/stj_sts.csv\").drop_duplicates(\n\u001B[0m\u001B[1;32m      2\u001B[0m     \u001B[0msubset\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"sentence_A\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"sentence_B\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkeep\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"last\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m )\n\u001B[1;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'TOTAL: '\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstj_sts_df\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'TREINO: '\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstj_sts_df\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mstj_sts_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSPLIT\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'TRAIN'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.9/site-packages/pandas/util/_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    309\u001B[0m                     \u001B[0mstacklevel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstacklevel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    310\u001B[0m                 )\n\u001B[0;32m--> 311\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    312\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    313\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[1;32m    676\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    677\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 678\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    679\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    680\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    573\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    574\u001B[0m     \u001B[0;31m# Create the parser.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 575\u001B[0;31m     \u001B[0mparser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    576\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    577\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m    930\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    931\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhandles\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mIOHandles\u001B[0m \u001B[0;34m|\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 932\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    933\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    934\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1214\u001B[0m             \u001B[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1215\u001B[0m             \u001B[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1216\u001B[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001B[0m\u001B[1;32m   1217\u001B[0m                 \u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1218\u001B[0m                 \u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.9/site-packages/pandas/io/common.py\u001B[0m in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    784\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencoding\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;34m\"b\"\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    785\u001B[0m             \u001B[0;31m# Encoding\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 786\u001B[0;31m             handle = open(\n\u001B[0m\u001B[1;32m    787\u001B[0m                 \u001B[0mhandle\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    788\u001B[0m                 \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../data/sts/stj_sts.csv'"
     ]
    }
   ],
   "source": [
    "stj_sts = pd.read_csv(\"../data/sts/stj_sts.csv\").drop_duplicates(\n",
    "    subset=[\"sentence_A\", \"sentence_B\"], keep=\"last\"\n",
    ")\n",
    "print('TOTAL: ', len(stj_sts_df))\n",
    "print('TREINO: ', len(stj_sts_df[stj_sts_df.SPLIT == 'TRAIN']))\n",
    "print('TESTE: ', len(stj_sts_df[stj_sts_df.SPLIT == 'TEST']))\n",
    "print('VALID: ', len(stj_sts_df[stj_sts_df.SPLIT == 'VALID']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## An√°lise de dados dos precedentes do TCU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tcu = pd.read_csv('data/raw_text/jurisprudencias_tcu_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Gero pares de uma mesma Jurisprud√™ncia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tcu_sts_data = []\n",
    "for group_name, jurisprudencia_group in tcu.groupby('ENUNCIADO'):\n",
    "    pares = list(set(itertools.combinations(jurisprudencia_group.VOTO, 2)))\n",
    "    noise = np.random.normal(mu, sigma, len(pares))\n",
    "    scores = [4.5]*len(pares)+noise\n",
    "    for i, par in enumerate(pares):\n",
    "        tcu_sts_data.append([par[0], par[1], scores[i], 4])\n",
    "\n",
    "precedentes_mesma_jurisprudencia = len(tcu_sts_data)\n",
    "print('Total de pares de precedentes da mesma Jurisprud√™ncia gerados: ', precedentes_mesma_jurisprudencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Gero pares de √Åreas diferentes, ent√£o a similaridade √© praticamente nula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pares_areas = list(set(itertools.combinations(tcu.AREA.unique().tolist(), 2)))\n",
    "print('Total de pares de diferentes √Åreas: ' , len(pares_areas))\n",
    "total_por_par_areas = int(precedentes_mesma_jurisprudencia/len(pares_areas))\n",
    "print('Total de pares a serem usados a partir de cada par de Mat√©ria: ', total_por_par_areas)\n",
    "for areas_par in list(set(itertools.combinations(tcu.AREA.unique().tolist(), 2))):\n",
    "    #recupero os votos das duas diferentes √°reas\n",
    "    precedentes_areas = [tcu[tcu.AREA == area].VOTO.tolist() for area in areas_par]\n",
    "    # gero um produto cartesiano entre os votos das diferentes √°reas\n",
    "    pares = list(itertools.product(*precedentes_areas))[:total_por_par_areas]\n",
    "    noise = np.random.normal(mu, sigma, len(pares))\n",
    "    scores = [0.5]*len(pares)+noise\n",
    "    for i, par in enumerate(pares):\n",
    "        tcu_sts_data.append([par[0], par[1], scores[i], 0])\n",
    "print('Total de pares at√© o momento: ', len(tcu_sts_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pares_temas = []\n",
    "for group_name, area_group in tcu.groupby('AREA'):\n",
    "    for tema_name, tema_group in area_group.groupby('TEMA'):\n",
    "        #Se pra um TEMA tenho mais que um subtema ent√£o a similaridade entre subtemas n√£o √© t√£o grande\n",
    "        if len(tema_group.SUBTEMA.unique()) > 1:\n",
    "            for subtemas_pares in list(set(itertools.combinations(tema_group.SUBTEMA.unique(), 2))):\n",
    "                precedentes_subtemas = [tema_group[tema_group.SUBTEMA == subtema].VOTO.tolist() \n",
    "                                        for subtema in subtemas_pares]\n",
    "                pares = list(itertools.product(*precedentes_subtemas))\n",
    "                noise = np.random.normal(mu, sigma, len(pares))\n",
    "                scores = [3]*len(pares)+noise\n",
    "                for i, par in enumerate(pares):\n",
    "                    pares_temas.append([par[0], par[1], scores[i], 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "samples_pares_temas = random.sample(pares_temas, min(precedentes_mesma_jurisprudencia, len(pares_temas)))\n",
    "\n",
    "tcu_sts_data = tcu_sts_data + samples_pares_temas\n",
    "print('Total de pares at√© o momento', len(tcu_sts_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tcu_sts_df = pd.DataFrame(tcu_sts_data, columns=['sentence_A','sentence_B','score','range'])\n",
    "tcu_sts_df = tcu_sts_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(tcu_sts_df, \n",
    "                            test_size=0.3, \n",
    "                            stratify=tcu_sts_df.range,\n",
    "                            shuffle=True,\n",
    "                            random_state=42)\n",
    "test, valid = train_test_split(test, \n",
    "                            test_size=0.3, \n",
    "                            stratify=test.range,\n",
    "                            shuffle=True,\n",
    "                            random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tcu_sts_df.loc[train.index.values,'SPLIT'] = 'TRAIN'\n",
    "tcu_sts_df.loc[test.index.values,'SPLIT'] = 'TEST'\n",
    "tcu_sts_df.loc[valid.index.values,'SPLIT'] = 'VALID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(tcu_sts_df)\n",
    "tcu_sts_df.to_csv('data/sts/tcu_sts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tcu_sts = pd.read_csv(\"data/sts/tcu_sts.csv\").drop_duplicates(\n",
    "    subset=[\"sentence_A\", \"sentence_B\"], keep=\"last\"\n",
    ")\n",
    "print('TOTAL: ', len(tcu_sts_df))\n",
    "print('TREINO: ', len(tcu_sts_df[tcu_sts_df.SPLIT == 'TRAIN']))\n",
    "print('TESTE: ', len(tcu_sts_df[tcu_sts_df.SPLIT == 'TEST']))\n",
    "print('VALID: ', len(tcu_sts_df[tcu_sts_df.SPLIT == 'VALID']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}